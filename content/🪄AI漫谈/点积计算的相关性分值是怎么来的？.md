---
{"publish":true,"cssclasses":""}
---

在[[🪄AI漫谈/注意力机制在 Transformer 模型中是如何运作的？\|注意力机制]]中，查询（Q）和键（K）向量之间的点积用于计算相似度分数：

Score=dk​​Q⋅K​

高分表示词元之间具有相关性。虽然点积很高效，但对于长序列而言，其二次方的复杂度（O(n2)）促使了对[[稀疏注意力]]等替代方案的研究。


我们已经知道：**注意力权重 ← 相关性得分 ← 点积计算**。

那么，这个作为基础的“**点积相关性得分**”本身，它的意义是从哪里来的呢？为什么两个向量的点积（dot product）就能代表“相关性”？

答案是：**这个“相关性”不是凭空产生的，而是模型通过学习，将“语义相关性”巧妙地编码成了“向量空间上的几何关系”。点积只是一个衡量这种几何关系的尺子。**

### 第一性原理剖析：点积得分的意义来源

想象一个巨大的、多维的“**意义空间**”，我们所有的词语都在这个空间里，像星星一样有自己的位置。

1. **初始状态（词嵌入 Word Embedding）**： 在进入注意力机制之前，每个词有一个初始的向量表示，叫**词嵌入**。比如，“猫 (cat)” 和 “老鼠 (mouse)” 在这个空间里的位置可能比较近，因为它们都是动物。这是它们的基础含义。我们把这个初始向量叫做 `x`。
    
2. **学习“角色扮演”的剧本（学习 Wq 和 Wk 矩阵）**： 训练的目标，就是学习到两个神奇的“**投影仪**”：
    
    - **查询投影仪 (Wq)**：它能把任何一个词的初始向量 `x`，投影到“**提问者空间**”里，变成一个**查询向量 (Query)**。这个投影会放大这个词作为“提问者”的特性。
    - **标签投影仪 (Wk)**：它能把任何一个词的初始向量 `x`，投影到“**被查询者空间**”里，变成一个**键向量 (Key)**。这个投影会放大这个词作为“标签”的特性。
    
    **关键点**：`Wq` 和 `Wk` 是两套**不同**的、通过海量训练学到的参数。
    
3. **训练的魔力：让相关性 = 几何上的接近** 模型训练的“指挥棒”（损失函数）会迫使 `Wq` 和 `Wk` 这两个“投影仪”学会一种默契：
    
    - 当一个词需要寻找它的**动作发出者**时（比如“追”作为Query），`Wq`投影仪会把它投影到“意义空间”中一个叫做“**寻找主语**”的特定方向。
    - 同时，`Wk`投影仪必须学会，把那些**能当主语的词**（比如“猫”），也投影到“意义空间”中“**寻找主语**”这个方向附近。
    - 对于那些**不能当主语的词**（比如“和”、“的”），`Wk`投影仪会把它们投影到离这个方向很远的地方。
    
    经过亿万次训练，模型就学会了：**只要两个词在语义上是相关的（比如一个是动作，另一个是它的主语），那么就把它们的Query向量和Key向量投影到空间中相互靠近、方向一致的地方。**
    
### 视觉化解析：点积的几何直觉

现在，点积运算的意义就显现出来了。在几何上，两个向量 `A` 和 `B` 的点积是 `A · B = |A| |B| cos(θ)`。

- `|A|` 和 `|B|`是向量的长度。
- `cos(θ)` 是它们之间夹角的余弦。

当两个向量**方向几乎完全一致**时，夹角 `θ` 趋近于0，`cos(θ)` 趋近于1，点积得到一个**很大的正数**。 当两个向量**方向垂直无关**时，夹角 `θ` 是90度，`cos(θ)` 是0，点积为**0**。 当两个向量**方向完全相反**时，夹角 `θ` 是180度，`cos(θ)` 是-1，点积得到一个**很大的负数**。

**所以，点积计算就是在用数学的方式问：“‘追’这个词的Query向量，和‘猫’这个词的Key向量，在方向上有多么一致？”**

- 因为训练好的模型已经把“追(Query)”和“猫(Key)”投影到了空间中方向一致的地方，所以它们的点积**得分很高**。
- 模型同样把“追(Query)”和“的(Key)”投影到了风马牛不及的地方，所以它们的点积**得分很低**。

<iframe
    height = 1100
    width = 80%
    padding = 0 0
    margins = 0 0
    src="https://pkm365.github.io/pages/dotproduct.html"></iframe>
### 总结

回到你的问题：“点积计算的相关性分值，它又是怎么来的？”

1. **它来自于Query向量和Key向量**：分数的高低直接由这两个向量决定。
2. **Query和Key向量来自于原始词向量与Wq、Wk矩阵的乘积**：是模型为每个词在特定场景下分配的“角色”。
3. **Wq和Wk矩阵来自于海量数据的训练**：这是整个魔法的核心。训练过程以“预测准确”为最终目标，**倒逼**着模型去学习如何生成Q和K向量，使得它们的**几何对齐关系（通过点积衡量）能够完美地映射现实世界中的语义相关性**。

所以，这个“相关性分值”不是一个先验的、被定义的规则，而是模型为了完成任务，自己学会的一种内部语言。它发现，将世界的关系编码成几何关系，再用点积这把尺子去度量，是一条通往成功的康庄大道。
