---
{"publish":true,"title":"How dot product work in attention mechanics","tags":["ZK/PN"],"cssclasses":""}
---

## 点积（Dot Product）在自注意力机制中是如何起作用的？

点积（Dot Product）在自注意力机制中扮演着**核心的相似度计算器**的角色。它的主要作用是衡量在某个特定上下文中，一个词（的 Query）应该对另一个词（的 Key）投入多少关注。

其工作流程可以概括为以下步骤：

1. **准备输入**：在进行点积计算之前，句子中的每一个词向量都已经被转换成了三个不同的角色向量[[🪄AI漫谈/查询（query）、键（key）和值（value）]]。我们这里主要关心其中的两个：
    
    - **查询向量 (Query, Q)**：代表当前词为了更好地理解自己，需要去“查询”的信息。
    - **键向量 (Key, K)**：代表一个词能够提供的“标签”信息，以供其他词来查询匹配。
2. **执行计算**：对于当前我们正在处理的词（例如，句子中的第 `i` 个词），模型会拿出它的 **Q 向量**（`Qi`），然后与句子中**所有**词（包括它自己）的 **K 向量**（`K1, K2, ..., Kn`）逐一进行点积运算。
    
    - `Score_i,j = Qi · Kj`
3. **产生输出**：每一次点积运算都会产生一个标量（一个数字），这个数字被称为**原始注意力得分（Raw Attention Score）**。这个分值直接反映了 Query 和 Key 两个向量之间的关系。
    
4. **解释得分**：
    
    - 从几何上看，这个得分的意义在于衡量两个向量的方向一致性。一个高的正分意味着 Q 和 K 向量在多维空间中指向相似的方向。
    - 这个原始得分并不是最终的权重，它只是一个中间结果。
5. **后续步骤**：所有计算出的原始得分会汇集在一起，经过 注意力得分的缩放因子 (sqrt(dk))处理，然后被送入 Softmax 函数进行归一化，最终才得到真正的注意力权重。

**小结**：简而言之，点积在自注意力机制中的作用就是：**作为一个高效的计算工具，通过衡量 Q 和 K 向量的对齐程度，为后续生成最终注意力权重提供原始的、未经处理的相似度分数。**
## 360

[[🪄AI漫谈/点积计算的相关性分值是怎么来的？]]

